
@Article{e24111514,
AUTHOR = {Ji, Tianbo and Lyu, Chenyang and Jones, Gareth and Zhou, Liting and Graham, Yvette},
TITLE = {QAScoreâ€”An Unsupervised Unreferenced Metric for the Question Generation Evaluation},
JOURNAL = {Entropy},
VOLUME = {24},
YEAR = {2022},
NUMBER = {11},
ARTICLE-NUMBER = {1514},
URL = {https://www.mdpi.com/1099-4300/24/11/1514},
PubMedID = {36359608},
ISSN = {1099-4300},
ABSTRACT = {Question Generation (QG) aims to automate the task of composing questions for a passage with a set of chosen answers found within the passage. In recent years, the introduction of neural generation models has resulted in substantial improvements of automatically generated questions in terms of quality, especially compared to traditional approaches that employ manually crafted heuristics. However, current QG evaluation metrics solely rely on the comparison between the generated questions and references, ignoring the passages or answers. Meanwhile, these metrics are generally criticized because of their low agreement with human judgement. We therefore propose a new reference-free evaluation metric called QAScore, which is capable of providing a better mechanism for evaluating QG systems. QAScore evaluates a question by computing the cross entropy according to the probability that the language model can correctly generate the masked words in the answer to that question. Compared to existing metrics such as BLEU and BERTScore, QAScore can obtain a stronger correlation with human judgement according to our human evaluation experiment, meaning that applying QAScore in the QG task benefits to a higher level of evaluation accuracy.},
DOI = {10.3390/e24111514}
}